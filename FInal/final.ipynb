{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ece5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, ast, re\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b676f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_interpreter = tf.lite.Interpreter(model_path=\"converted_currency_mobilenetmodel.tflite\")\n",
    "watermark_interpreter = tf.lite.Interpreter(model_path=\"converted_watermark_mobilenetmodel.tflite\")\n",
    "uv_interpreter = tf.lite.Interpreter(model_path=\"converted_uv_mobilenetmodel.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691dd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_input_details = classify_interpreter.get_input_details()\n",
    "classify_output_details = classify_interpreter.get_output_details()\n",
    "watermark_input_details = watermark_interpreter.get_input_details()\n",
    "watermark_output_details = watermark_interpreter.get_output_details()\n",
    "uv_input_details = uv_interpreter.get_input_details()\n",
    "uv_output_details = uv_interpreter.get_output_details()\n",
    "\n",
    "classify_interpreter.allocate_tensors()\n",
    "watermark_interpreter.allocate_tensors()\n",
    "uv_interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff3e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path):\t\n",
    "\tf = open(path, \"r\")\n",
    "\tlabels = f.read()\n",
    "\tlabels = ast.literal_eval(labels)\n",
    "\n",
    "\tfinal_labels = {v: k for k, v in labels.items()}\n",
    "\t\n",
    "\treturn final_labels\n",
    "currency_labels = get_labels('mobilenet_currency_class_indices.txt')\n",
    "uv_labels = get_labels('mobilenet_uv_class_indices.txt')\n",
    "watermark_labels = get_labels('mobilenet_watermark_class_indices.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7029f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(imgname, final_labels, interpreter, input_details, output_details):\n",
    "    test_image = cv2.imread(imgname)\n",
    "    test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "    if final_labels==uv_labels:\n",
    "        h, w, c = test_image.shape #height*width*channel - ex: 480,640,3\n",
    "        x_start_r = 0.3125  \n",
    "        x_end_r = 0.703125  \n",
    "        y_start_r = 0.270833\n",
    "        y_end_r = 0.791666  \n",
    "    \n",
    "        #np.floor-round of lowest integer value\n",
    "        x = int(np.floor(x_start_r * w))#200\n",
    "        y = int(np.floor(y_start_r * h))# 130\n",
    "        x2 = int(np.floor(x_end_r * w))#450\n",
    "        y2 = int(np.floor(y_end_r * h))# 380\n",
    "\n",
    "        test_image = test_image[y:y2, x:x2].copy() #copying image to image parameter\n",
    "        #[130:380,200:450]-croping image for considering security thread\n",
    "    else:\n",
    "        test_image = cv2.resize(test_image, (224, 224), cv2.INTER_AREA)\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    test_image = (2.0 / 255.0) * test_image - 1.0\n",
    "    test_image = test_image.astype(np.float32)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "    interpreter.invoke()\n",
    "    result = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    result_dict = dict()\n",
    "    for key in list(final_labels.keys()):\n",
    "        result_dict[final_labels[key]] = result[0][key]\n",
    "    sorted_results = {k: v for k, v in sorted(result_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    final_result = dict()\n",
    "    final_result[list(sorted_results.keys())[0]] = sorted_results[list(sorted_results.keys())[0]] * 100\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2383da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Bec\\Projects\\BEC_Project\\4-2\\Project\\CNN-based-classification-(PRIMARY)\\batch-test-images\\10back.jpg\n",
      "{'10_new_back_up': 34.22751426696777}\n",
      "Real\n"
     ]
    }
   ],
   "source": [
    "path_classify=input()\n",
    "res_classify=predict_image(path_classify, currency_labels, classify_interpreter, classify_input_details, classify_output_details)\n",
    "if res_classify.get(\"invalid\") is not None:\n",
    "    print(res_classify)\n",
    "    a=cv2.imread(path_classify)\n",
    "    cv2.imshow('fake note',a)\n",
    "    cv2.waitKey(0)\n",
    "    print(\"Fake\")\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(res_classify)\n",
    "    a=cv2.imread(path_classify)\n",
    "    cv2.imshow('real note',a)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2664cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Bec\\Projects\\BEC_Project\\4-2\\Project\\CNN-based-classification-(PRIMARY)\\test-images\\OT-2-11.jpg\n",
      "{'invalid': 11.735772341489792}\n",
      "Fake\n"
     ]
    }
   ],
   "source": [
    "path_classify=input()\n",
    "res_classify=predict_image(path_classify, currency_labels, classify_interpreter, classify_input_details, classify_output_details)\n",
    "if res_classify.get(\"invalid\") is not None:\n",
    "    print(res_classify)\n",
    "    a=cv2.imread(path_classify)\n",
    "    cv2.imshow('fake note',a)\n",
    "    cv2.waitKey(0)\n",
    "    print(\"Fake\")\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(res_classify)\n",
    "    a=cv2.imread(path_classify)\n",
    "    cv2.imshow('real note',a)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5c6042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Bec\\Projects\\BEC_Project\\4-2\\Project\\CNN-based-UV-validation-(PRIMARY)\\test-images\\fake.jpg\n",
      "{'nopatch': 90.15783071517944}\n",
      "Fake\n"
     ]
    }
   ],
   "source": [
    "path_uv=input()\n",
    "res_uv=predict_image(path_uv, uv_labels, uv_interpreter, uv_input_details, uv_output_details)\n",
    "if res_uv.get(\"nopatch\") is not None:\n",
    "    print(res_uv)\n",
    "    a=cv2.imread(path_uv)\n",
    "    cv2.imshow('fake note',a)\n",
    "    cv2.waitKey(0)\n",
    "    print(\"Fake\")\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(res_uv)\n",
    "    a=cv2.imread(path_uv)\n",
    "    cv2.imshow('real note',a)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f7c6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Bec\\Projects\\BEC_Project\\4-2\\Project\\CNN-based-watermark-validation-(PRIMARY)\\test-images\\yesww.jpg\n",
      "{'yes_watermark': 99.86935257911682}\n",
      "Real\n"
     ]
    }
   ],
   "source": [
    "path_wm=input()\n",
    "res_wm=predict_image(path_wm, watermark_labels, watermark_interpreter, watermark_input_details, watermark_output_details)\n",
    "if res_wm.get(\"no_watermark\") is not None:\n",
    "    print(res_wm)\n",
    "    a=cv2.imread(path_wm)\n",
    "    cv2.imshow('fake note',a)\n",
    "    cv2.waitKey(0)\n",
    "    print(\"Fake\")\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(res_wm)\n",
    "    a=cv2.imread(path_wm)\n",
    "    cv2.imshow('real note',a)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72401901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500_new_back_down: 16.78403466939926%\n",
      "invalid: 8.921962976455688%\n",
      "100_new_back_down: 6.527264416217804%\n",
      "20_new_back_down: 5.860710516571999%\n",
      "20_new_back_up: 5.613524466753006%\n",
      "200_new_back_down: 5.180063843727112%\n",
      "20_old_front_up: 4.490919038653374%\n",
      "2000_new_back_up: 3.7207994610071182%\n",
      "10_old_front_up: 2.977895736694336%\n",
      "500_new_front_down: 2.5883732363581657%\n",
      "50_old_back_up: 2.3997852578759193%\n",
      "50_new_back_down: 2.2311674430966377%\n",
      "10_old_back_down: 2.002578414976597%\n",
      "200_new_front_up: 1.6380209475755692%\n",
      "10_old_back_up: 1.6289569437503815%\n",
      "500_new_back_up: 1.6174331307411194%\n",
      "50_new_front_down: 1.505236979573965%\n",
      "50_old_front_up: 1.4186342246830463%\n",
      "200_new_back_up: 1.3539457693696022%\n",
      "100_old_front_down: 1.289104949682951%\n",
      "20_old_front_down: 1.2757597491145134%\n",
      "20_old_back_up: 1.241471990942955%\n",
      "50_old_front_down: 1.2031812220811844%\n",
      "50_new_front_up: 1.1993111111223698%\n",
      "50_old_back_down: 1.1987831443548203%\n",
      "100_new_front_down: 1.1152454651892185%\n",
      "50_new_back_up: 1.0489551350474358%\n",
      "10_new_back_up: 1.0398178361356258%\n",
      "2000_new_back_down: 1.0382832027971745%\n",
      "2000_new_front_down: 1.0297253727912903%\n",
      "100_new_front_up: 1.0192086920142174%\n",
      "10_new_back_down: 0.9980902075767517%\n",
      "20_new_front_up: 0.948171503841877%\n",
      "20_old_back_down: 0.8621431887149811%\n",
      "200_new_front_down: 0.6867913994938135%\n",
      "100_new_back_up: 0.662179384380579%\n",
      "500_new_front_up: 0.617979047819972%\n",
      "10_old_front_down: 0.6032295059412718%\n",
      "100_old_back_up: 0.5106749013066292%\n",
      "20_new_front_down: 0.4791059996932745%\n",
      "100_old_front_up: 0.3830142319202423%\n",
      "100_old_back_down: 0.36043208092451096%\n",
      "10_new_front_down: 0.34889811649918556%\n",
      "2000_new_front_up: 0.2567555522546172%\n",
      "10_new_front_up: 0.12237795162945986%\n",
      "==================================================\n",
      "{'500_new_back_down': 16.78403466939926}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, ast, re\n",
    "import cv2\n",
    "\n",
    "def get_labels(path):\t\n",
    "\tf = open(path, \"r\")\n",
    "\tlabels = f.read()\n",
    "\tlabels = ast.literal_eval(labels)\n",
    "\n",
    "\tfinal_labels = {v: k for k, v in labels.items()}\n",
    "\t\n",
    "\treturn final_labels\n",
    "\n",
    "def predict_image(imgname, final_labels, interpreter, input_details, output_details):\n",
    "    test_image = cv2.imread(imgname)\n",
    "    test_image = cv2.resize(test_image, (224, 224), cv2.INTER_AREA)\n",
    "    cv2.imshow('kappa', test_image)\n",
    "    cv2.waitKey(0)\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "    interpreter.invoke()\n",
    "    result = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    result_dict = dict()\n",
    "    for key in list(final_labels.keys()):\n",
    "        result_dict[final_labels[key]] = result[0][key]\n",
    "    sorted_results = {k: v for k, v in sorted(result_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    for label in sorted_results.keys():\n",
    "        print(\"{}: {}%\".format(label, sorted_results[label] * 100))\n",
    "\n",
    "    final_result = dict()\n",
    "    final_result[list(sorted_results.keys())[0]] = sorted_results[list(sorted_results.keys())[0]] * 100\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "classify_interpreter = tf.lite.Interpreter(model_path=\"converted_currency_mobilenetmodel.tflite\")\n",
    "watermark_interpreter = tf.lite.Interpreter(model_path=\"converted_watermark_mobilenetmodel.tflite\")\n",
    "uv_interpreter = tf.lite.Interpreter(model_path=\"converted_uv_mobilenetmodel.tflite\")\n",
    "\n",
    "classify_input_details = classify_interpreter.get_input_details()\n",
    "classify_output_details = classify_interpreter.get_output_details()\n",
    "watermark_input_details = watermark_interpreter.get_input_details()\n",
    "watermark_output_details = watermark_interpreter.get_output_details()\n",
    "uv_input_details = uv_interpreter.get_input_details()\n",
    "uv_output_details = uv_interpreter.get_output_details()\n",
    "\n",
    "classify_interpreter.allocate_tensors()\n",
    "watermark_interpreter.allocate_tensors()\n",
    "uv_interpreter.allocate_tensors()\n",
    "\n",
    "currency_labels = get_labels('mobilenet_currency_class_indices.txt')\n",
    "uv_labels = get_labels('mobilenet_uv_class_indices.txt')\n",
    "watermark_labels = get_labels('mobilenet_watermark_class_indices.txt')\n",
    "\n",
    "res = predict_image(r'D:\\Bec\\Projects\\BEC_Project\\4-2\\Project\\CNN-based-classification-(PRIMARY)\\test-images\\2.jpg', currency_labels, classify_interpreter, classify_input_details, classify_output_details)\n",
    "print(\"=\" * 50)\n",
    "print(res)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
